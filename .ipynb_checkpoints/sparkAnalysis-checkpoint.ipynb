{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8591795a-ae64-4c6b-a62b-a72e7e40fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ad6082-7419-48c3-af9e-7ed4a9d396fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d257f1a-5658-4b4d-b91b-32f94447866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb5f6464-6f07-4a58-99f6-a4a261d3770f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\Spark3\\python\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+--------+--------------------+--------------------+------------+--------------------+------------------+------------------+\n",
      "|arrest_date|arrest_time|sex|              race|searched|      reason_stopped|     search_based_on|search_found|          race_known|               lng|               lat|\n",
      "+-----------+-----------+---+------------------+--------+--------------------+--------------------+------------+--------------------+------------------+------------------+\n",
      "| 2015-01-01|          2|  M|HISPANIC OR LATINO| YES = 1|    CALL FOR SERVICE|INCIDENTAL TO ARREST|     NOTHING|NO - RACE OR ETHN...|-97.73419151256908| 30.26646917024065|\n",
      "| 2015-01-01|        317|  M|             WHITE| YES = 1|VIOLATION OF TRAN...|INCIDENTAL TO ARREST|       DRUGS|NO - RACE OR ETHN...| -97.7776889200335|30.227662552230814|\n",
      "| 2015-01-01|        317|  F|             WHITE| YES = 1|VIOLATION OF TRAN...|INCIDENTAL TO ARREST|       DRUGS|NO - RACE OR ETHN...| -97.7776889200335|30.227662552230814|\n",
      "| 2015-01-01|        426|  M|             WHITE| YES = 1|VIOLATION OF TRAN...|INCIDENTAL TO ARREST|     NOTHING|NO - RACE OR ETHN...|-97.76017254544549|30.274213186235304|\n",
      "| 2015-01-01|        106|  F|             WHITE| YES = 1|  CONSENSUAL CONTACT|INCIDENTAL TO ARREST|       OTHER|NO - RACE OR ETHN...|-97.73972017814472| 30.26624565910905|\n",
      "+-----------+-----------+---+------------------+--------+--------------------+--------------------+------------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import our input dataset\n",
    "df = pd.read_csv('https://profilingbucket.s3.us-east-2.amazonaws.com/combined_cleaned_data.csv')\n",
    "\n",
    "profiling_arrest_analysis = pd.DataFrame(df)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pandasToSpark\").getOrCreate()\n",
    "\n",
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(profiling_arrest_analysis)\n",
    "\n",
    "# Show the Spark DataFrame\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a33780df-d7a9-4979-8811-71015e386b69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arrest_date', 'string'),\n",
       " ('arrest_time', 'bigint'),\n",
       " ('sex', 'string'),\n",
       " ('race', 'string'),\n",
       " ('searched', 'string'),\n",
       " ('reason_stopped', 'string'),\n",
       " ('search_based_on', 'string'),\n",
       " ('search_found', 'string'),\n",
       " ('race_known', 'string'),\n",
       " ('lng', 'double'),\n",
       " ('lat', 'double')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1776175-d0c4-4b1c-9cb2-014345999f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+----+--------+--------------+---------------+------------+----------+---+---+\n",
      "|arrest_date|arrest_time|sex|race|searched|reason_stopped|search_based_on|search_found|race_known|lng|lat|\n",
      "+-----------+-----------+---+----+--------+--------------+---------------+------------+----------+---+---+\n",
      "|          0|          0|  0|   0|       0|             0|              0|           0|         0|  0|  0|\n",
      "+-----------+-----------+---+----+--------+--------------+---------------+------------+----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, col, when\n",
    "\n",
    "# Check for Null Values/'\n",
    "spark_df.select([count(when(col(c).isNull(), c)).alias(c) for c in spark_df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d48b518-34e7-4aba-b9ac-e77dbc681d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Convert 'arrest_date' and 'arrest_time' to timestamp\n",
    "spark_df = spark_df.withColumn('arrest_date', to_timestamp(col('arrest_date')))\n",
    "spark_df = spark_df.withColumn('arrest_time', to_timestamp(col('arrest_time')))\n",
    "\n",
    "# Convert 'lng' and 'lat' to double\n",
    "spark_df = spark_df.withColumn('lng',spark_df['lng'].cast('double'))\n",
    "spark_df = spark_df.withColumn('lat',spark_df['lat'].cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395a577b-c29d-45a0-8030-afb800a20306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrest_date: timestamp (nullable = true)\n",
      " |-- arrest_time: timestamp (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- searched: string (nullable = true)\n",
      " |-- reason_stopped: string (nullable = true)\n",
      " |-- search_based_on: string (nullable = true)\n",
      " |-- search_found: string (nullable = true)\n",
      " |-- race_known: string (nullable = true)\n",
      " |-- lng: double (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Get the data types of the columns. \n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45e8c069-b4c9-41ca-a37e-c0a02e0e087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a579249e-b7f7-4a51-9486-6689ae20492a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o119.json.\n: ExitCodeException exitCode=-1073741515: \r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:774)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save Spark DataFrame as JSON file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maustinArrests.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Read JSON file into Pandas DataFrame\u001b[39;00m\n\u001b[0;32m      6\u001b[0m pandas_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(json_path, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Spark\\Spark3\\python\\pyspark\\sql\\readwriter.py:1658\u001b[0m, in \u001b[0;36mDataFrameWriter.json\u001b[1;34m(self, path, mode, compression, dateFormat, timestampFormat, lineSep, encoding, ignoreNullFields)\u001b[0m\n\u001b[0;32m   1649\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1651\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1652\u001b[0m     dateFormat\u001b[38;5;241m=\u001b[39mdateFormat,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1656\u001b[0m     ignoreNullFields\u001b[38;5;241m=\u001b[39mignoreNullFields,\n\u001b[0;32m   1657\u001b[0m )\n\u001b[1;32m-> 1658\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\Spark3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\Spark\\Spark3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\Spark3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o119.json.\n: ExitCodeException exitCode=-1073741515: \r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1007)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:774)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "# Save Spark DataFrame as JSON file\n",
    "json_path = \"austinArrests.json\"\n",
    "spark_df.write.json(json_path)\n",
    "\n",
    "# Read JSON file into Pandas DataFrame\n",
    "pandas_df = pd.read_json(json_path, orient=\"records\")\n",
    "\n",
    "# Display the Pandas DataFrame\n",
    "print(pandas_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95e10f-25dd-42b1-b657-f34e929a5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pyspark\n",
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640befb-4d1f-4317-8f08-4aceb8bc53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "\n",
    "spark_df = spark_df.withColumn(\"arrest_time\", from_utc_timestamp(\"arrest_time\", \"UTC\"))\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bed52b-9635-4920-8b7b-e66830623c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2753dc-ac61-4e66-96f3-bcff8324746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.select(\"arrest_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5747de-8ff4-4d89-861f-ced2770deb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab8c14-5e53-4747-bb9f-85df1e8025fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Assuming \"arrest_time\" values are in seconds since the Unix epoch\n",
    "spark_df = spark_df.withColumn(\"arrest_time\", expr(\"timestamp(from_utc_timestamp(arrest_time, 'UTC'))\"))\n",
    "\n",
    "# Print the DataFrame schema and check the \"arrest_time\" values again\n",
    "print(spark_df.schema)\n",
    "spark_df.select(\"arrest_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05e4ca-0edf-4697-a2d0-d0e0d833ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Drop time zone information\n",
    "spark_df = spark_df.withColumn(\"arrest_time\", col(\"arrest_time\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b16b1-d233-4bee-b2ac-977a50fccb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = spark_df.toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdda7b2-346f-498c-af4f-44bc8c77f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert 'arrest_time' column to 'UTC' time zone in Pandas\n",
    "pandas_df[\"arrest_time\"] = pandas_df[\"arrest_time\"].dt.tz_localize('UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354910c-34ed-46cf-b6b2-1e3761208f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_utc_timestamp\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"UTC\").getOrCreate()\n",
    "\n",
    "# Convert timestamp to UTC\n",
    "spark_df = spark_df.withColumn(\"arrest_time\", from_utc_timestamp(\"arrest_time\", \"UTC\"))\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pandas_df = spark_df.toPandas()\n",
    "\n",
    "# Save Pandas DataFrame as JSON file\n",
    "json_path = \"austinArrests.json\"\n",
    "pandas_df.to_json(json_path, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493b6e6-c5bc-41af-9ebc-889642d6a8a7",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "#### 1. Aggregations by Month or Day:\n",
    "\r\n",
    "The query on arrests by month provides insights into the distribution of arrests across different demographic groups over each month. Here are some findings based on the provided result:\r\n",
    "\r\n",
    "Monthly Distribution:\r\n",
    "\r\n",
    "The data shows the distribution of arrests across different months.\r\n",
    "Each row represents a combination of sex, race, and the respective month.\r\n",
    "Highest Arrest Months:\r\n",
    "\r\n",
    "January (month 1) appears to have higher arrest counts across various demographic groups.\r\n",
    "For example, Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO), White Males (sex=M, race=WHITE), and Black Males (sex=M, race=BLACK) have relatively high arrest counts in January.\r\n",
    "Variation Across Demographic Groups:\r\n",
    "\r\n",
    "Arrest counts vary across sex and race categories for each month.\r\n",
    "Different demographic groups may have distinct patterns in terms of arrests, potentially influenced by various factors.\r\n",
    "Low Arrest Counts:\r\n",
    "\r\n",
    "Some demographic groups, especially in minority categories, may have lower arrest counts in certain months.\r\n",
    "Data Exploration:\r\n",
    "\r\n",
    "Further exploration could involve visualizing the monthly trends using line charts or other time series visualizations.\r\n",
    "Analyzing whether certain events or seasons contribute to the observed patterns.\r\n",
    "Consideration for Seasonality:\r\n",
    "\r\n",
    "Patterns in arrests could be influenced by seasonal factors, holidays, or specific events that occur at different times of the year.\r\n",
    "It's important to note that while these findings provide insights into the monthly distribution of arrests, a more detailed analysis, possibly with visualizations, could reveal additional patterns and context. Domain knowledge and understanding the context of the dataset would contribute to a more comprehensive interpretation of the findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83322515-4b90-4b35-8377-a33f3e134d38",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "- This analysis aims to understand the patterns of arrests by examining the demographic attributes of gender (sex), racial or ethnic background (race), and the temporal aspect represented by monthly trends.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa1840-207e-4808-96b2-9788d96442bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a temporary SQL table\n",
    "spark_df.createOrReplaceTempView(\"arrest_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c46494-688d-4064-8bbc-1bc84e946a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, dayofmonth\n",
    "\n",
    "result_by_month = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        month(arrest_date) AS arrest_month,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race, arrest_month\n",
    "    ORDER BY\n",
    "        arrest_month, arrest_count DESC\n",
    "\"\"\")\n",
    "print('This analysis aims to understand the patterns of arrests by examining the demographic attributes of gender (sex), racial or ethnic background (race), and the temporal aspect represented by monthly trends.')\n",
    "result_by_month.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36343b-5055-4da8-9e7a-da832ef1e223",
   "metadata": {},
   "source": [
    "#### 2. Temporal Trends:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986566bf-7004-41ec-a23f-d8321aacdcba",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "\r\n",
    "The query on temporal trends analyzes the distribution of arrests over time across different demographic groups (combinations of sex and race). Here are some findings based on the provided result:\r\n",
    "\r\n",
    "Temporal Distribution:\r\n",
    "\r\n",
    "The data shows the temporal trends in arrests from January 1, 2015, onward.\r\n",
    "Arrests on Specific Dates:\r\n",
    "\r\n",
    "The table reveals the number of arrests on specific dates for different demographic groups.\r\n",
    "For example, on January 1, 2015, there were arrests for Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO), Black Males (sex=M, race=BLACK), White Males (sex=M, race=WHITE), and White Females (sex=F, race=WHITE).\r\n",
    "Variation Across Dates:\r\n",
    "\r\n",
    "The count of arrests varies across different dates and demographic groups.\r\n",
    "Temporal Patterns:\r\n",
    "\r\n",
    "Patterns in arrests may emerge over time, reflecting factors such as day of the week, holidays, or specific events.\r\n",
    "Data Exploration:\r\n",
    "\r\n",
    "Further exploration of the data could involve visualizing temporal trends using line charts or other time series visualizations.\r\n",
    "Identifying spikes or patterns on specific dates may lead to insights or hypotheses about the reasons behind those trends.\r\n",
    "Consideration for Time of Day:\r\n",
    "\r\n",
    "It might be beneficial to explore temporal patterns not only by date but also by the time of day to uncover patterns related to specific hours.\r\n",
    "It's important to note that a more detailed analysis, possibly with visualizations, could provide a clearer understanding of the temporal trends in arrests for different demographic groups. Additionally, domain knowledge and context about the dataset could contribute to a more comprehensive interpretation of the findings.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952bb9b-c9f3-4651-a18a-1c53951e130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_temporal_trends = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        arrest_date,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race, arrest_date\n",
    "    ORDER BY\n",
    "        arrest_date, arrest_count DESC\n",
    "\"\"\")\n",
    "print('This query analyzes temporal trends by providing the count for each combination of sex, race, and arrest date.')\n",
    "result_temporal_trends.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb8ee4b-e199-41a2-b073-25441d879530",
   "metadata": {},
   "source": [
    "#### 3. Reasons for Arrest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ea67a-27f3-447b-b6c4-5b4395de0276",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "\n",
    "\r\n",
    "The query on reasons for arrest provides insights into the distribution of different reasons for arrests across various demographic groups (combinations of sex and race). Here are some findings based on the provided result:\r\n",
    "\r\n",
    "Top Reasons for Arrest:\r\n",
    "\r\n",
    "For Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO), the most common reasons for arrest are \"Moving Traffic Violation\" (5,231 arrests) and \"VIOLATION OF TRANSPORTATION CODE/VEHICLE LAWS\" (4,367 arrests).\r\n",
    "For Black Males and White Males, \"Moving Traffic Violation\" and \"VIOLATION OF TRANSPORTATION CODE/VEHICLE LAWS\" are also among the top reasons.\r\n",
    "Traffic Violations:\r\n",
    "\r\n",
    "\"Moving Traffic Violation\" and \"VIOLATION OF TRANSPORTATION CODE/VEHICLE LAWS\" appear to be prominent reasons for arrest across different demographic groups.\r\n",
    "Violation of Law Other Than Traffic:\r\n",
    "\r\n",
    "Another category is \"Violation of law other than traffic,\" which contributes to a significant number of arrests for Hispanic or Latino Males and White Males.\r\n",
    "Not Listed Reasons:\r\n",
    "\r\n",
    "Some records have reasons for arrest marked as \"not_listed,\" indicating that the specific reason is not provided.\r\n",
    "Gender and Race Dynamics:\r\n",
    "\r\n",
    "The reasons for arrest may vary based on both gender and race, and the analysis can help identify patterns and potential areas for further investigation.\r\n",
    "Law Enforcement Practices:\r\n",
    "\r\n",
    "The findings may reflect law enforcement practices, and further analysis could involve exploring the context behind the arrests in each category.\r\n",
    "As with any analysis, the interpretation may benefit from additional context, domain knowledge, and visualizations. Consider visualizing the data to better understand the patterns and relationships between different demographic groups and the reasons for their arrests.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140903e3-fd2a-4217-8fea-cc3e455a0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_reasons_for_arrest = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        reason_stopped,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race, reason_stopped\n",
    "    ORDER BY\n",
    "        arrest_count DESC\n",
    "\"\"\")\n",
    "print('This query explores the reasons for arrest, providing the count for each combination of sex, race, and reason_stopped.')\n",
    "result_reasons_for_arrest.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e09f1b-a7e6-4a1d-b108-001e6d3d2b9a",
   "metadata": {},
   "source": [
    "#### 4. Search Outcomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732855c-c35b-4cf6-a79a-d55eb11cf564",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "\n",
    "The query on search outcomes provides insights into the distribution of different search outcomes for various demographic groups (combinations of sex and race). Here are some findings based on the provided result:\r\n",
    "\r\n",
    "Common Search Outcomes:\r\n",
    "\r\n",
    "The column search_found represents the outcomes of searches, such as \"NOTHING,\" \"not_listed,\" \"DRUGS,\" \"OTHER,\" \"CASH,\" and so on.\r\n",
    "The majority of searches across all demographic groups resulted in \"NOTHING.\"\r\n",
    "Demographic Patterns:\r\n",
    "\r\n",
    "Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO) have the highest count of searches resulting in \"NOTHING\" (7,561), followed by White Males and Black Males.\r\n",
    "Different demographic groups may exhibit varying patterns in search outcomes.\r\n",
    "Variation in Outcomes:\r\n",
    "\r\n",
    "There is a variety of outcomes, including \"not_listed,\" \"DRUGS,\" \"OTHER,\" and \"CASH.\" These outcomes may provide insights into the reasons for searches and the subsequent results.\r\n",
    "Potential Areas of Interest:\r\n",
    "\r\n",
    "Analyzing the outcomes of searches can be crucial for understanding law enforcement practices and identifying any disparities in treatment based on demographic factors.\r\n",
    "Further Analysis:\r\n",
    "\r\n",
    "It would be valuable to explore the reasons behind different search outcomes, especially for cases where searches did not result in findings (\"NOTHING\") or when outcomes are not explicitly listed (\"not_listed\").\r\n",
    "As with any analysis, the interpretation may be enhanced with additional context, domain knowledge, and visualizations. Consider exploring visual representations of these search outcomes to better understand the patterns and identify any areas that require closer examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8eaac-4073-4934-8bbe-6c47f61dba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_search_outcomes = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        search_found,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race, search_found\n",
    "    ORDER BY\n",
    "        arrest_count DESC\n",
    "\"\"\")\n",
    "print('This query investigates outcomes of searches by providing the count for each combination of sex, race, and search_found.')\n",
    "result_search_outcomes.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0b107-1461-4031-9fa7-08947ca0adf3",
   "metadata": {},
   "source": [
    "#### 5. Geospatial Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d600f2-20e4-424f-be60-b47a48053fd2",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "\r\n",
    "The geospatial analysis query provides insights into the average geographical locations (longitude and latitude) and arrest counts for different demographic groups (combinations of sex and race). Here are the findings based on the provided result:\r\n",
    "\r\n",
    "Average Geographical Locations:\r\n",
    "\r\n",
    "The columns avg_lng and avg_lat represent the average longitude and latitude, respectively, for each combination of sex and race.\r\n",
    "For example, Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO) have an average longitude of approximately 7.40 and an average latitude of approximately 85.78.\r\n",
    "Arrest Counts:\r\n",
    "\r\n",
    "The column arrest_count represents the total number of arrests for each combination of sex and race.\r\n",
    "For example, Hispanic or Latino Males have the highest arrest count with 13,613 arrests, followed by White Males with 9,408 arrests and Black Males with 7,713 arrests.\r\n",
    "Variation in Geographical Locations:\r\n",
    "\r\n",
    "There is variation in the average geographical locations across different demographic groups. This could be indicative of different patterns of arrests in different areas.\r\n",
    "Demographic Distribution:\r\n",
    "\r\n",
    "The table provides a breakdown of arrests, considering both demographic factors (sex and race) and geographical factors (average longitude and latitude).\r\n",
    "Potential Insights:\r\n",
    "\r\n",
    "Further analysis could involve visualizing these geospatial patterns on a map to identify clusters or trends in arrests for specific demographic groups.\r\n",
    "Keep in mind that these findings are based on the provided data, and the interpretation may vary based on the context and domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ee2530-5811-4f0c-aec3-786f233806e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_geospatial_analysis = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        AVG(lng) AS avg_lng,\n",
    "        AVG(lat) AS avg_lat,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race\n",
    "    ORDER BY\n",
    "        arrest_count DESC\n",
    "\"\"\")\n",
    "print('This query performs geospatial analysis by providing the average longitude, latitude, and count for each combination of sex and race.')\n",
    "result_geospatial_analysis.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ad070-4c6a-4c1e-b667-fdcc0f03eeb3",
   "metadata": {},
   "source": [
    "#### 6. Searches Based On:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017808d1-de16-4a2b-bc24-774db5bd1c03",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "\n",
    "Result Explanation:\r\n",
    "\r\n",
    "The result provides a tabular representation of the distribution of different reasons for conducting searches, broken down by gender, race, and search reason.\r\n",
    "Each row represents a unique combination of gender, race, and search reason, and the corresponding count of arrests for that combinatio\n",
    "ple Insiata):\r\n",
    "\r\n",
    "The table might show, for instance, that there are a certain number of arrests for searches conducted based on \"Probable Cause\" for Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO).\r\n",
    "Another row might indicate the count of arrests where searches were conducted based on \"Consent\" for White Females (sex=F, race=WHITE).\r\n",
    "This analysis allows for an understanding of how searches are distributed across different demographic groups and the reasons for conducting those searches within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fc537-6b37-4aed-83a7-88a0d60c85ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_searches_based_on = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        search_based_on,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race, search_based_on\n",
    "    ORDER BY\n",
    "        arrest_count DESC\n",
    "\"\"\")\n",
    "print('This query analyzes the distribution of different reasons for conducting searches, providing the count for each combination of sex, race, and search_based_on.')\n",
    "result_searches_based_on.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3a575-f1dd-4d91-a983-ea1569943528",
   "metadata": {},
   "source": [
    "#### 7. Demographic Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e9191-72aa-4892-821e-04971b6269fc",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: Observations:\r\n",
    "\r\n",
    "The table provides a detailed breakdown of arrests, allowing for an analysis of law enforcement interactions with individuals based on their gender and race.\r\n",
    "The counts represent the frequency of arrests for each demographic grou\n",
    "ple Insiata):\r\n",
    "\r\n",
    "Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO):\r\n",
    "Arrest Count: 13,613.\r\n",
    "White Females (sex=F, race=WHITE):\r\n",
    "Arrest Count: 4,176.\r\n",
    "Black Males (sex=M, race=BLACK):\r\n",
    "Arrest Count: 7,713.\r\n",
    "Considerations:\r\n",
    "\r\n",
    "Use this data for demographic profiling and understanding the distribution of arrests across different groups.\r\n",
    "Identify any disparities in arrest counts based on gender and race.\r\n",
    "This dataset can be a starting point for further analysis, such as examining arrest rates, trends over time, or geographic patterns.\r\n",
    "Limitations:\r\n",
    "\r\n",
    "The table provides a count of arrests but does not include additional contextual information.\r\n",
    "Demographic analysis should be approached with caution to avoid perpetuating stereotypes or biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cee207-654a-49b6-9f64-52fe48191aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_demographic_analysis = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race\n",
    "    ORDER BY\n",
    "        arrest_count DESC\n",
    "\"\"\")\n",
    "print('This query performs demographic analysis by providing the count for each combination of sex and race.')\n",
    "result_demographic_analysis.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd41b9-8025-4396-a5d6-a55c008777b7",
   "metadata": {},
   "source": [
    "#### 8. Average Arrest Location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835e41f-45f2-44ef-b238-ff0bed1e70cf",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "Observations:\r\n",
    "Geographical Insights:\r\n",
    "\r\n",
    "The table allows you to compare the average arrest locations for different demographic groups.\r\n",
    "For example, Hispanic or Latino males (sex=M, race=HISPANIC OR LATINO) have an average arrest location with a longitude of approximately 7.40 and a latitude of approximately 85.78.\r\n",
    "Temporal Insights:\r\n",
    "\r\n",
    "The maximum and minimum arrest dates provide insights into the temporal distribution of arrests within each demographic group.\r\n",
    "For example, the data suggests that arrests for Hispanic or Latino males span from January 1, 2015, to December 31, 2020.\r\n",
    "Arrest Volumes:\r\n",
    "\r\n",
    "The total number of records indicates the arrest volume for each demographic group.\r\n",
    "For example, Hispanic or Latino males have the highest total arrest count, with approximately 13,\n",
    "\n",
    "Interpretation\n",
    "\n",
    "Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO):\r\n",
    "\r\n",
    "Average arrest location: Longitude 7.40, Latitude 85.78.\r\n",
    "Arrest date range: January 1, 2015, to December 31, 2020.\r\n",
    "Total arrest count: 13,613.\r\n",
    "White Females (sex=F, race=WHITE):\r\n",
    "\r\n",
    "Average arrest location: [Average values for longitude and latitude].\r\n",
    "Arrest date range: [Date range].\r\n",
    "Total arrest count: [Count].\r\n",
    "Considerations:\r\n",
    "Comparative Analysis:\r\n",
    "\r\n",
    "Use the table to compare arrest patterns between different demographic groups.\r\n",
    "Temporal Trends:\r\n",
    "\r\n",
    "Explore how arrest patterns have evolved over time within each demographic category.\r\n",
    "Geographical Disparities:\r\n",
    "\r\n",
    "Investigate if there are geographical disparities in arrests across different gender and race groups.\r\n",
    "Limitations:\r\n",
    "Geographical Precision:\r\n",
    "\r\n",
    "The average longitude and latitude represent central points and may not capture the full spatial variability within a demographic category.\r\n",
    "Temporal Resolution:\r\n",
    "\r\n",
    "The date range provides an overview but might not capture finer temporal details.\r\n",
    "This table serves as a valuable tool for understanding the spatial and temporal dimensions of arrests within different demographic groups, enabling further analysis and insights into law enforcement activities.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "613 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6cd40-4841-4067-bc84-f0298f8caecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run SQL queries on the DataFrame\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        AVG(lng) AS avg_lng,\n",
    "        AVG(lat) AS avg_lat,\n",
    "        MAX(arrest_date) AS max_arrest_date,\n",
    "        MIN(arrest_date) AS min_arrest_date,\n",
    "        COUNT(*) AS total_records\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race\n",
    "\"\"\")\n",
    "print('The table allows us to compare and contrast different demographic groups based on their average geographical location, arrest date ranges, and arrest volumes.')\n",
    "# Show the result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e532ee13-da5e-40b9-993f-78b127565f78",
   "metadata": {},
   "source": [
    "### 9. Geographical Distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6944ce49-2b7b-4ece-9562-6ea9121538d6",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "\n",
    "\n",
    "Key Insights:\r\n",
    "Location Information:\r\n",
    "\r\n",
    "The table includes the average longitude and latitude values for each gender and racial or ethnic category, providing an indication of the central location associated with arrests.\r\n",
    "Arrest Counts (arrest_count):\r\n",
    "\r\n",
    "Indicates the total number of arrests for each combination of gender and race.\r\n",
    "The table is ordered by the arrest count in descending order.\r\n",
    "Observations:\r\n",
    "Dominant Contributors to Arrests:\r\n",
    "\r\n",
    "The table shows which gender and racial or ethnic categories contribute the most to the total arrest count.\r\n",
    "In this example, Hispanic or Latino males have the highest average arrest count, followed by white males and black males.\r\n",
    "Geographical Context:\r\n",
    "\r\n",
    "The average longitude and latitude values provide an approximate central location associated with arrests for each demogr\n",
    "aInterpretation \n",
    "Hispanic or Latino Males (sex=M, race=HISPANIC OR LATINO):\r\n",
    "\r\n",
    "Highest average arrest count.\r\n",
    "Associated with a specific geographical location, characterized by an average longitude of approximately 7.40 and an average latitude of approximately 85.78.\r\n",
    "White Males (sex=M, race=WHITE):\r\n",
    "\r\n",
    "Second-highest average arrest count.\r\n",
    "Associated with a different geographical location, characterized by an average longitude of approximately 7.41 and an average latitude of approximately 85.80.\r\n",
    "Black Males (sex=M, race=BLACK):\r\n",
    "\r\n",
    "Third-highest average arrest count.\r\n",
    "Associated with another geographical location, characterized by an average longitude of approximately 7.77 and an average latitude of approximately 85.78.\r\n",
    "Considerations:\r\n",
    "Spatial Distribution:\r\n",
    "\r\n",
    "The table allows for an exploration of the spatial distribution of arrests, highlighting areas where certain demographic groups are more frequently arrested.\r\n",
    "Potential Hotspots:\r\n",
    "\r\n",
    "Areas with higher average arrest counts may indicate potential hotspots for law enforcement activities related to specific demographic categories.\r\n",
    "Limitations:\r\n",
    "Geographical Precision:\r\n",
    "\r\n",
    "The provided averages represent central points and might not accurately capture the spatial variability within a demographic category.\r\n",
    "Context and External Factors:\r\n",
    "\r\n",
    "The analysis is based solely on arrest counts and geographical averages, and external factors influencing arrest locations are not considered here.\r\n",
    "This table serves as a tool for understanding the geographical distribution of arrests across different demographic categories, offering insights into potential patterns and disparities in law enforcement activities.ic category.\r\n",
    "Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e8345-e008-4f78-994b-52d91d03b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_location = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        AVG(lng) AS avg_lng,\n",
    "        AVG(lat) AS avg_lat,\n",
    "        COUNT(*) AS arrest_count\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race\n",
    "    ORDER BY\n",
    "        arrest_count DESC\n",
    "\"\"\")\n",
    "\n",
    "result_location.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225956e-dfd6-496a-b5c2-eb985aa03643",
   "metadata": {},
   "source": [
    "### 10. highlighting the count and percentage of arrests based on 'sex' and 'race'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277bccd-8114-42e2-b625-91f3a3fb38b9",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "Hispanic or Latino Males:\r\n",
    "\r\n",
    "Males of Hispanic or Latino ethnicity contribute the highest percentage (32.86%) to the total arrest count, indicating a significant presence in the dataset.\r\n",
    "White and Black Males:\r\n",
    "\r\n",
    "White males (22.71%) and black males (18.62%) are the next two significant contributors to the total arrest count.\r\n",
    "Gender Disparities:\r\n",
    "\r\n",
    "The table provides insights into potential gender disparities in arrests, with a breakdown of contributions from both males and females.\r\n",
    "Considerations:\r\n",
    "Limitations:\r\n",
    "\r\n",
    "The analysis is based on the available data and may not capture the entire context of arrests. Further exploration and context-specific knowledge may be needed.\r\n",
    "Policy and Social Implications:\r\n",
    "\r\n",
    "Discussions around the percentages can lead to considerations of policy implications and potential areas for further examination of law enforcement practices.\r\n",
    "This table serves as a summary of the distribution of arrests across different demographic categories, offering insights into the relative contributions of each group to the overall arrest count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a15ae2-18af-4554-a9bd-a508a1f5e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_percentages = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        COUNT(*) AS arrest_count,\n",
    "        (COUNT(*) / SUM(COUNT(*)) OVER ()) * 100 AS arrest_percentage\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race\n",
    "    ORDER BY\n",
    "        arrest_count DESC\n",
    "\"\"\")\n",
    "\n",
    "result_with_percentages.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171144a7-260d-4042-b21e-085cda8b256b",
   "metadata": {},
   "source": [
    "#### 11. Percentage by year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb4a5b-d734-48d0-984d-99e4a81b3baf",
   "metadata": {},
   "source": [
    "#### Notes for findings and Visualiszations: \n",
    "\n",
    "This table serves as a valuable tool for understanding how arrests are distributed across gender and racial or ethnic categories over multiple years, offering insights into potential trends or shifts in law enforcement practices.\n",
    "\n",
    "\n",
    "Yearly Distribution:\r\n",
    "\r\n",
    "The table is segmented by arrest year, providing insights into changes in arrest patterns over time.\r\n",
    "Dominant Contributors Each Year:\r\n",
    "\r\n",
    "For each year, the table shows which gender and racial or ethnic categories contribute the most to the total arrest c\n",
    "\n",
    "##### Considerations:\n",
    "Yearly Trends:\r\n",
    "\r\n",
    "The table facilitates the examination of trends in arrests over time, providing insights into whether certain demographic categories consistently contribute more to arrests or if there are variations.\r\n",
    "Potential Changes in Enforcement:\r\n",
    "\r\n",
    "Substantial changes in percentages from one year to the next might indicate shifts in law enforcement priorities, policies, or practices.\r\n",
    "Intersectionality:\r\n",
    "\r\n",
    "The intersection of gender and race allows for a nuanced understanding of arrest patterns, considering the unique experiences of different demographic groups.ount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a1b25-0a0a-42e3-a0b0-534ac91657af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_by_year = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        sex,\n",
    "        race,\n",
    "        YEAR(arrest_date) AS arrest_year,\n",
    "        COUNT(*) AS arrest_count,\n",
    "        (COUNT(*) / SUM(COUNT(*)) OVER (PARTITION BY YEAR(arrest_date))) * 100 AS arrest_percentage\n",
    "    FROM\n",
    "        arrest_data\n",
    "    GROUP BY\n",
    "        sex, race, arrest_year\n",
    "    ORDER BY\n",
    "        arrest_year, arrest_count DESC\n",
    "\"\"\")\n",
    "\n",
    "result_by_year.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a9c8e-dcb1-4fe7-9f0a-283e34fbb0db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
