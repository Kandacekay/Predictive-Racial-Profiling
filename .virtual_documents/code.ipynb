# Import necessary libraries
import pandas as pd
from pyproj import Proj, Transformer
from pathlib import Path
import warnings


# Ignore FutureWarnings from pyproj
warnings.simplefilter(action='ignore', category=FutureWarning)


# Read the CSV file into a Pandas DataFrame
arrest_2020 = pd.read_csv('data/2020_Racial_Profiling__RP__dataset_20231109.csv')

# Filter records for 'Arrests' in the 'Type' column
arrests_2020 = arrest_2020[arrest_2020['Type'] == 'Arrests']

# Display the resulting DataFram
arrests_2020.to_csv('arrests_ONLY_2020_.csv', index=False)


# Define the desired column order
desired_column_order = ['arrest_date', 'arrest_time', 'sex', 'race', 'searched', 'reason_stopped', 'search_based_on',
                        'search_found', 'race_known', 'lng', 'lat']

# Function to process the DataFrame for a specific year
def process_dataframe(df, year):
    # Create a copy of the DataFrame
    df_copy = df.copy()

    # Convert 'X_COORDINATE' (lng) and 'Y_COORDINATE' (lat) columns to actual latitude and longitude
    if 'X_COORDINATE' in df_copy.columns and 'Y_COORDINATE' in df_copy.columns:
        # Replace non-numeric values with NaN
        df_copy[['X_COORDINATE', 'Y_COORDINATE']] = df_copy[['X_COORDINATE', 'Y_COORDINATE']].apply(pd.to_numeric, errors='coerce')

        # Define the projection (assuming the original coordinates are in EPSG:3081)
        in_proj = Proj(init='epsg:3081')
        out_proj = Proj(init='epsg:4326')  # WGS 84

        # Convert coordinates using the updated Pyproj method
        transformer = Transformer.from_proj(in_proj, out_proj)
        df_copy['lng'], df_copy['lat'] = transformer.transform(df_copy['X_COORDINATE'].values, df_copy['Y_COORDINATE'].values)

        # Drop the original 'X_COORDINATE' and 'Y_COORDINATE' columns
        df_copy = df_copy.drop(['X_COORDINATE', 'Y_COORDINATE'], axis=1)

    # Drop specified columns based on the year
    drop_columns = {
        2015: ['PRIMARY_KEY', 'LOCATION', 'SECTOR', 'LOCAL_FIELD1'],
        2016: ['PRIMARY_KEY', 'LOCATION', 'SECTOR', 'LOCAL_FIELD1'],
        2018: ['PRIMARY_KEY', 'LOCATION', 'APD_sector', 'CouncilDistrict', 'county_description', 'ZIP', 'CENSUS_TRACT'],
        2019: ['PRIMARY_KEY', 'LOCATION', 'APD_sector', 'CouncilDistrict', 'county_description', 'ZIP', 'CENSUS_TRACT'],
        2020: ['Stop Key', 'Type', 'TCOLE RACE ETHNICITY', 'Street_Type', 'TCOLE Result of Stop',
               'TCOLE Arrest Based On', 'CENSUS_TRACT', 'Council District', 'COUNTY', 'Custody', 'Location', 'Sector', 'Zip Code']
    }
    df_copy = df_copy.drop(columns=drop_columns.get(year, []), errors='ignore')

    # Rename columns based on the year
    rename_columns = {
        2015: {'REP_DATE': 'arrest_date', 'REP_TIME': 'arrest_time', 'SEX': 'sex', 'APD_RACE_DESC': 'race',
               'PERSON_SEARCHED_DESC': 'searched', 'REASON_FOR_STOP_DESC': 'reason_stopped',
               'SEARCH_BASED_ON_DESC': 'search_based_on', 'SEARCH_DISC_DESC': 'search_found', 'RACE_KNOWN': 'race_known',
               'X_COORDINATE': 'lng', 'Y_COORDINATE': 'lat'},
        2016: {'REP_DATE': 'arrest_date', 'REP_TIME': 'arrest_time', 'SEX': 'sex', 'APD_RACE_DESC': 'race',
               'PERSON_SEARCHED_DESC': 'searched', 'REASON_FOR_STOP_DESC': 'reason_stopped',
               'SEARCH_BASED_ON_DESC': 'search_based_on', 'SEARCH_DISC_DESC': 'search_found', 'RACE_KNOWN': 'race_known',
               'X_COORDINATE': 'lng', 'Y_COORDINATE': 'lat'},
        2018: {'REP_DATE': 'arrest_date', 'REP_TIME': 'arrest_time', 'X_COORDINATE': 'lng',
               'Y_COORDINATE': 'lat', 'RACE_KNOWN': 'race_known', 'Reason for Stop – TCOLE form': 'reason_stopped',
               'SEX': 'sex', 'APD_RACE_DESC': 'race', 'Person Search YN': 'searched', 'Search Based On': 'search_based_on',
               'Search Found': 'search_found'},
        2019: {'REP_DATE': 'arrest_date', 'REP_TIME': 'arrest_time', 'X_COORDINATE': 'lng',
               'Y_COORDINATE': 'lat', 'RACE_KNOWN': 'race_known', 'Reason for Stop – TCOLE form': 'reason_stopped',
               'SEX': 'sex', 'APD_RACE_DESC': 'race', 'Person Search YN': 'searched', 'Search Based On': 'search_based_on',
               'Search Found': 'search_found'},
        2020: {'TCOLE Sex': 'sex', 'Standardized Race Known': 'race_known',
               'Reason for Stop': 'reason_stopped', 'Search Yes or No': 'searched',
               'TCOLE Search Based On': 'search_based_on', 'TCOLE Search Found': 'search_found',
               'Standardized Race': 'race', 'Stop Date': 'arrest_date', 'Stop Time': 'arrest_time', 'X_COORDINATE': 'lng',
               'Y_COORDINATE': 'lat'},
    }
    df_copy = df_copy.rename(columns=rename_columns.get(year, {}))

    # Replace missing or null values with "unknown"
    columns_to_replace = ['sex', 'race', 'reason_stopped', 'search_based_on', 'search_found', 'race_known', 'searched']
    df_copy[columns_to_replace] = df_copy[columns_to_replace].fillna('not_listed')

    # Convert columns to appropriate data types
    df_copy['arrest_date'] = pd.to_datetime(df_copy['arrest_date'])
    df_copy['arrest_time'] = pd.to_datetime(df_copy['arrest_time'], errors='coerce')
    df_copy['sex'] = df_copy['sex'].astype('category')
    df_copy['race'] = df_copy['race'].astype('category')
    df_copy['searched'] = df_copy['searched'].astype('category')
    df_copy['reason_stopped'] = df_copy['reason_stopped'].astype('category')
    df_copy['search_based_on'] = df_copy['search_based_on'].astype('category')
    df_copy['search_found'] = df_copy['search_found'].astype('category')
    df_copy['race_known'] = df_copy['race_known'].astype('category')

    # Convert 'lng' and 'lat' columns to numeric, handling errors and coercion to NaN
    for col in ['lng', 'lat']:
        df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')
        # Fill NaN values with the mean
        df_copy[col] = df_copy[col].fillna(df_copy[col].mean())
        # Convert columns to the appropriate data type
        df_copy[col] = df_copy[col].astype('float64')

    df_copy = df_copy[desired_column_order]
    return df_copy


# Dictionary to store DataFrames
dfs = {}

# List of CSV file paths
file_paths = {
    2015: "data/2015_Racial_Profiling_Arrests_20231113.csv",
    2016: "data/2016_RP_Arrests_20231113.csv",
    2018: "data/2018_RP_Arrests.csv",
    2019: "data/2019_Racial_Profiling__RP__Arrests_20231113.csv",
    2020: "data/arrests_ONLY_2020_.csv",
}

# Loop through each file, read data, and add DataFrame to the dictionary
for year, file_path in file_paths.items():
    # Read the CSV file into a Pandas DataFrame
    profiling_df = pd.read_csv(Path(file_path))
    
    # Process the DataFrame for the specific year
    processed_df = process_dataframe(profiling_df, year)

    # Add the processed DataFrame to the dictionary
    dfs[year] = processed_df

    # Check data types
    print(f"\nDTYPES FOR {year} DF:\n")
    print(processed_df.dtypes)

# Display the first few rows of each DataFrame
for year, df in dfs.items():
    print(f"\nDF for the year {year}:\n")
    display(df.head(3))



# Define the directory to save the CSV files
output_directory = "data/cleaned_data"
# "C:\github_Repos\Predictive-Racial-Profiling\data\cleaned_data"
# Loop through each DataFrame, save it to a CSV file
for year, df in dfs.items():
    # Construct the file path
    output_file_path = Path(output_directory, f"{year}_cleaned_data.csv")

    # Save the DataFrame to a CSV file
    df.to_csv(output_file_path, index=False)

    print(f"DataFrame for the year {year} saved to: {output_file_path}")



# Combine all DataFrames into a single DataFrame
combined_df = pd.concat(dfs.values(), ignore_index=True)

# Display the first few rows of the combined DataFrame
print("\nCombined DataFrame:\n")
display(combined_df.head(3))

# Save the combined DataFrame to a CSV file
combined_output_path = Path(output_directory, "combined_cleaned_data.csv")
combined_df.to_csv(combined_output_path, index=False)

print(f"\nCombined DataFrame saved to: {combined_output_path}")


# Initialize an empty DataFrame for combination
df_combined = pd.DataFrame()

# Concatenate DataFrames into a combined DataFrame
selected_columns = ['arrest_date', 'arrest_time', 'sex', 'race', 'reason_stopped', 'search_based_on', 'search_found', 'race_known', 'lng', 'lat', 'searched']

df_combined = pd.concat([dfs[year][selected_columns] for year in dfs.keys()], ignore_index=True)



# Find null values in the DataFrame
null_values = df_combined.isnull().sum()

# Display the columns with null values and their counts
print("Columns with Null Values:")
print(null_values[null_values > 0])


# List Categorical variables
categorical_columns = ['sex', 'race', 'reason_stopped', 'search_based_on', 'search_found', 'race_known']

# Drop unnecessary columns
df_combined = df_combined.drop(['arrest_date', 'arrest_time'], axis=1)

# Convert 'searched' column to binary (1 for YES, 0 for NO)
df_combined['searched'] = df_combined['searched'].apply(lambda x: 1 if x == 'YES' else 0)

# Convert categorical variables to numerical using one-hot encoding
df_combined = pd.get_dummies(df_combined, columns=categorical_columns, drop_first=False)

# Convert boolean columns to integers (0 or 1) for the entire DataFrame
df_combined = df_combined.astype(int)

# Manually remove the 'sex_F' column
df_combined = df_combined.drop('sex_F', axis=1)


from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

X = df_combined.drop('searched', axis=1)
y = df_combined['searched']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


X


# Initialize the logistic regression model
model = LogisticRegression(max_iter=1000, random_state=42)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the standard scaler
scaler = StandardScaler()

# Fit the scaler on the training data and transform both training and test data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the model on the scaled data
model.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_scaled)

# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))



from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("example").config("spark.driver.memory", "2g").getOrCreate()

# Create a Spark session
spark = SparkSession.builder.appName("example").getOrCreate()

# Read the CSV file into a DataFrame without including the header
store_df = spark.read.option('header',True).option(“mode”,’DROPMALFORMED’).csv('data/cleaned_data/combined_cleaned_data.csv')

# Filter out the header row
filtered_df = store_df.filter(store_df['_c1'] != 'arrest_time')

# Show the DataFrame
filtered_df.show()


from pyspark.sql.functions import col
null_counts = filtered_df.select([col(c).isNull().alias(c) for c in filtered_df.columns])

# Filter rows with at least one null value
rows_with_null = null_counts.filter(null_counts.rdd.reduce(lambda x, y: x + y) > 0)

# Show the DataFrame with rows containing null values
rows_with_null.show()



